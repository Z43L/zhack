# recon

todo tiene su comienzo y en el hacking ocurre lo mismo antes de pretender atacar  lo que sea hay que haber echo un reconocimento de la empresa en la que se basa en recopilar informacion sobre todo lo relaccionado con la empresa como adquisiones de otras empresas que haya comprado subdominios urls  puertos servicios que estan corriendo en esos puertos informacion sobre los trabajadores para ingieneria social correos electronicos a fin de cuentas todo lo que tenga que ver con el objetivo a atacar ya que nos puede servir de gran utilidad cuando pasemos a  la parte de la explotacionen este capitulo cubriremos las tecnicas y herramienta necesarias para llevar acabo la investigacionlo primero por lo que deberiamos empezar es por saber cuales son las adquisiociones que tiene la empresa para ello puede resultar util hacer una primera busqueda en internet y leer un poco hacerca de la empresa aunque hay aplicaciones que nos dicen cuales son las adquisiciones de dicha empresa siempres es bueno echar nosotros mismos un vistazo en internet a ver que encontramos pero la forma rapida y precisa de saber que adquisiciones tiene una empresa son recurrir a aplicaciones como:

* Crunchbase: es una plataforma que ofrece información sobre empresas, incluyendo adquisiciones, financiamiento y otras métricas relevantes.
* PitchBook: es una herramienta de investigación de empresas que proporciona datos sobre fusiones y adquisiciones, capital de riesgo, financiamiento y más.
* Mergermarket: es una plataforma de inteligencia empresarial que ofrece información sobre fusiones y adquisiciones a nivel global.
* Bloomberg: es una plataforma financiera que proporciona información sobre empresas, incluyendo adquisiciones y otros datos financieros relevantes.
* Thomson Reuters Eikon: es una plataforma de información financiera que ofrece noticias, análisis y datos sobre empresas, incluyendo adquisiciones.



Con eso nos podremos hacer una idea del alcanze de la empresa y podriamos pasar a buscar los subdominios  de esa empresa y de sus adquisiones para ello ensisten 2 tipos de busqueda la pasiva que  consiste en el uso de informacion que esta publica en internet por lo que no es ilegal ni mucho menos ya que solo estas recopilando informacion que esta publica en internet es una forma de no hacerse notar a la empresa de que la estas intentando atacar por otro lado tenemos la busqueda activa de subdominios que al contrario que la otra si que deja rastro ya que consiste normalmente en por medio de fuerza bruta lanzando peticiones  contra el dominio en busca de subdominios lo que pasa con esta esque no pasa desapercibido y es muy ruidoso ya que son solucitudes que se enstan enviando al servidor y pueden ser detectadas pero es una forma muy util y eficaz de enumerar subdominios empezemos con las herramientas de enumeracion de subdominos pasivas:

[sublist3r ](https://github.com/aboul3la/Sublist3r)-> Es una herramienta para detectar subdominios pasivamente que  enumera subdominios usando Netcraft, Virustotal, ThreatCrowd, DNSdumpster y ReverseDNS.

```
sublist3r -d dominio.com 
```

esto lanzaria un escareo normal contra un dominio.

```
sublist3r -d dominio.com -b -v -t 50
```

lanzamos un escaneo contra un dominio con el -b hacemos que sea por fuerza bruta  y con -v aplicamos el modo verbose que es para que nos de mas informacion y con el -t especificamos cuantas cosas que se hagan simultaneamente

```
sublist3r -d dominio.com -mi Netcraft, Virustotal, ThreatCrowd -o sublist3r-dominio.txt
```

estamos  lanzado sublist3r contra un dominio pero con el -mi especificamos que tecnologia que use para enumerar los subdominios y con -o le indicamos que guarde el resultado en sublis2r-dominio.txt

* [Amass ](https://github.com/OWASP/Amass)-> realiza el mapeo de redes de superficies de ataque y el descubrimiento de activos externos utilizando técnicas de reconocimiento activo y recopilación de información de fuente abierta.

```
amass intel [options] [-whois -d DOMAIN] [-addr ADDR -asn ASN -cidr CIDR]
```

El subcomando Amass intel, o el módulo si lo desea, puede ayudarlo a recopilar inteligencia de fuente abierta sobre la organización y permitirle encontrar más nombres de dominio raíz asociados con la organización.

```
amass intel -d owasp.org -whois
```

este comando recopilara informacion del dominio de la como lo haria whois y ipv4info

```
amass intel -org facebook
```

aqui buscariamos informacion acerca del objetivo especificado

* [dnscan ](https://github.com/rbsec/dnscan)-> dnscan es un escáner de subdominio DNS basado en listas de palabras de Python.

```
dnscan.py (-d <dominio> | -l <lista>) -w wordlist
```

con este comando lanzamos dnscan contra un dominio  o contra una lista de dominos y especificamos un wordlist del que va a probar los subdominios

```
dnscan.py -d dominio.com --threads 50 --zonetransfer --ipv6
```

con ete comando especificamos la cantidad de hilos  le pedimos que haga una tranferencia de zona y que escanee por registrosregistrosregistrosregistrosregistrosregistrosregistrosregistros (AAAA)

### escaneo de puertos

* [masscan ](https://github.com/robertdavidgraham/masscan)-> Este es un escáner de puertos a escala de Internet. Puede escanear todo Internet en menos de 5 minutos, transmitiendo 10 millones de paquetes por segundo, desde una sola máquina.

NOTA: Masscan utiliza su propia **pila TCP/IP ad hoc** . Todo lo que no sea un simple escaneo de puertos puede causar un conflicto con la pila TCP/IP local. Esto significa que debe usar la `--src-ip`opción para ejecutar desde una dirección IP diferente, o usar `--src-port`para configurar qué puertos de origen usa Masscan, luego también configure el firewall interno (como `pf`o `iptables`) para proteger esos puertos del resto del sistema operativo

#### intalacion

```
sudo apt-get --assume-yes install git make gcc
git clone https://github.com/robertdavidgraham/masscan
cd masscan
make
make install
```

#### USO

para listar todos los comandos de masscan hay que hacer

```
masscan -h
usage:
masscan -p80,8000-8100 10.0.0.0/8 --rate=10000
 scan some web ports on 10.x.x.x at 10kpps
masscan --nmap
 list those options that are compatible with nmap
masscan -p80 10.0.0.0/8 --banners -oB <filename>
 save results of scan in binary format to <filename>
masscan --open --banners --readscan <filename> -oX <savefile>
 read binary scan results in <filename> and save them as xml in <savefile>

```

Para escanear un segmento de red en busca de algunos puertos

```
masscan -p80,8000-8100 10.0.0.0/8 2603:3001:2d00:da00::/112
```

Masscan puede hacer más que solo detectar si los puertos están abiertos. También puede completar la conexión TCP y la interacción con la aplicación en ese puerto para obtener información simple de "banner".

El problema con esto es que Masscan contiene su propia pila TCP/IP separada del sistema en el que lo ejecuta. Cuando el sistema local recibe un SYN-ACK del objetivo sondeado, responde con un paquete RST que elimina la conexión antes de que Masscan pueda tomar el banner.

La forma más fácil de evitar esto es asignar a Masscan una dirección IP separada. Esto se vería como uno de los siguientes ejemplos:

```
 masscan 10.0.0.0/8 -p80 --banners --source-ip 192.168.1.200
 masscan 2a00:1450:4007:810::/112 -p80 --banners --source-ip 2603:3001:2d00:da00:91d7:b54:b498:859d
```

n algunos casos, como WiFi, esto no es posible. En esos casos, puede proteger el puerto que usa Masscan. Esto evita que la pila TCP/IP local vea el paquete, pero Masscan aún lo ve, ya que pasa por alto la pila local. Para Linux, esto se vería así:

```
iptables -A INPUT -p tcp --dport 61000 -j DROP
masscan 10.0.0.0/8 -p80 --banners --source-port 61000
```

con masscan tambien se puede guardar toda la configuracion del comando para no tener que escribir una linea de comando tan larga

```
# My Scan
rate =  100000.00
output-format = xml
output-status = all
output-filename = scan.xml
ports = 0-65535
range = 0.0.0.0-255.255.255.255
excludefile = exclude.txt
```

Para utilizar este archivo de configuración, utilice el `-c`:

```
masscan -c myscan.conf
```

tambien podemos guardar el output del programa de la misma manera que en nmap `-o` segido de en que formato queremos guardarlo `-oX` para XML           `-oL` si queremos que nos lo guarde en modo lista pero hay mas formato grepeable, JSON, binario. y al final indicar el nombre del archivo en el que se va a guardar

```
masscan -c myscan.conf -oL myscan
```

[nmap ](https://github.com/nmap/nmap)-> nmap sirve para escanear puertos y servicios en un objetivo. Nmap tienes muchas obciones pero una forma facil de lanzar un escaneo es

```
nmap facebook.com
```

tambien se puede escanear atraves de la ip y hay diferentes tipos de escaneo empezando por que pueden ser por TCP que se lanza por defecto pero si lo queremos por UDP solo tenemos que añadir el parametro `-sU`&#x20;

```
nmap -sU --min-rate 5000 -v -n -p- 10.10.10.10 -oN nmapscan.txt
```

en esta linea de comando estamos lanzando un escaneo por UDP  con el parametro `--min-rate 5000` le estamos diciendo que lanze un escaneo que no vaya mas lento que 5000 paquetes por segundo con `-v` el modo verbose , con el `-n` le decimos que no aplique resolucion DNS  con `-p-` le indicamos que escanee los 65535 puertos le indicamos la ip a escanear y por ultimo con el parametro -oN guardamos el output del programa en formato normal en el archivo nmapscan.txt

```
namp -sS --min-rate 5000 -v -n -Pn -p- 10.10.10.10 -oN nmapscan.txt
```

este es el comando que suelo usar yo (CTF) en el que uso el `-sS` que  indica que quiero un tipo de escaneo que es SINCK que esto lo que sirve es para intentar eludir el firewall ya que deja menos rastro porque no llega a conectase la estructura del escaneo en lugar de  ser `SINCK- SINCK ACK - ACK` solo llega asta el `SINCK ACK` y no llega hacer el ACK que eso indica que se a conectado por lo que deja menos rastro y ayuda a eludir si hay algun firewall y el `-Pn` para que no me descubra virtual hosting pero solo lo uso esto en CTF ya que no me hace falta el virtual hosting

### captura de pantalla

* [EyeWitness](https://github.com/FortyNorthSecurity/EyeWitness) : EyeWitness está diseñado para tomar capturas de pantalla de sitios web, proporcionar información del encabezado del servidor e identificar las credenciales predeterminadas si es posible.

#### USO

```
EyeWitness.py -f filename --timeout optionaltimeout
```

```
eyewitness -h
usage: EyeWitness.py [--web] [-f Filename] [-x Filename.xml]
                     [--single Single URL] [--no-dns] [--timeout Timeout]
                     [--jitter # of Seconds] [--delay # of Seconds]
                     [--threads # of Threads]
                     [--max-retries Max retries on a timeout]
                     [-d Directory Name] [--results Hosts Per Page]
                     [--no-prompt] [--user-agent User Agent]
                     [--difference Difference Threshold]
                     [--proxy-ip 127.0.0.1] [--proxy-port 8080]
                     [--proxy-type socks5] [--show-selenium] [--resolve]
                     [--add-http-ports ADD_HTTP_PORTS]
                     [--add-https-ports ADD_HTTPS_PORTS]
                     [--only-ports ONLY_PORTS] [--prepend-https]
                     [--selenium-log-path SELENIUM_LOG_PATH]
                     [--resume ew.db]

```

una forma facil de usar es&#x20;

```
eyewitness -f url.txt --web
```

Esto lo que aria seria cojer una lista de url del archivo  y con `--web` tomar capturas de pantalla usando selenium otra forma de hacerlo es usando un archivo xml sustitullendo `-f` por `-x`  ademas le podemos añadir un numero maximo que hay entre captura de pantalla y otra por default esta en 7

```
eyewitness -x url.xml --timeout 8
```

otra forma de hacerlo seria usando un proxy para enviar la solicitud por ahi y por ejemplo capturarlo con burpsuite

```
eyewittness -f url.txt --web --proxy-ip 127.0.0.1 --proxy-port 8080 --proxy-type socks5 --timeout 120
```

* [aquatone ](https://github.com/michenriksen/aquatone)-> Aquatone es una herramienta para la inspección visual de sitios web en una gran cantidad de hosts y es conveniente para obtener rápidamente una descripción general de la superficie de ataque basada en HTTP.Aquatone es una herramienta para la inspección visual de sitios web en una gran cantidad de hosts y es conveniente para obtener rápidamente una descripción general de la superficie de ataque basada en HTTP.

#### USO

```
aquatone -h
  -chrome-path string
    	Full path to the Chrome/Chromium executable to use. By default, aquatone will search for Chrome or Chromium
  -debug
    	Print debugging information
  -http-timeout int
    	Timeout in miliseconds for HTTP requests (default 3000)
  -nmap
    	Parse input as Nmap/Masscan XML
  -out string
    	Directory to write files to (default ".")
  -ports string
    	Ports to scan on hosts. Supported list aliases: small, medium, large, xlarge (default "80,443,8000,8080,8443")
  -proxy string
    	Proxy to use for HTTP requests
  -resolution string
    	screenshot resolution (default "1440,900")
  -save-body
    	Save response bodies to files (default true)
  -scan-timeout int
    	Timeout in miliseconds for port scans (default 100)
  -screenshot-timeout int
    	Timeout in miliseconds for screenshots (default 30000)
  -session string
    	Load Aquatone session file and generate HTML report
  -silent
    	Suppress all output except for errors
  -template-path string
    	Path to HTML template to use for report
  -threads int
    	Number of concurrent threads (default number of logical CPUs)
  -version
    	Print current Aquatone version
```

para usar cimplemente hacemos un `cat`  a una lista de targets y con una tuberia lo pasamos por aquatone

```
cat targets.txt | aquatone
```

si no quieres que acuatone cree archivos en el directorio actual puesdes especificar una ubicacion diferente con el parametro `-out`

```
cat host.txt | aquatone -out ~/aquatone/objetivo.com
```

Por defecto, Aquatone escaneará los hosts de destino con una pequeña lista de puertos HTTP de uso común: 80, 443, 8000, 8080 y 8443. Puede cambiar esto a su propia lista de puertos con la bandera `-ports`:

```
cat hosts.txt | aquatone -ports 80,443,3000,3001
```

Aquatone puede generar un informe sobre los hosts escaneados con el escáner de puertos [Nmap](https://nmap.org/) o [Masscan](https://github.com/robertdavidgraham/masscan) . Simplemente alimente a Aquatone con la salida XML y dele la `-nmap`bandera para indicarle que analice la entrada como Nmap/Masscan XML:

```
cat scan.xml | aquatone -nmap
```

### Tecnologias

* [webanalyze ](https://github.com/rverton/webanalyze)-> descubre tecnologías utilizadas en sitios web para automatizar el escaneo masivo.&#x20;

#### Instalacion

```
go install -v github.com/rverton/webanalyze/cmd/webanalyze@latest
```

#### USO

```
webanalyze -h
Usage of webanalyze:
  -apps string
        app definition file. (default "technologies.json")
  -crawl int
        links to follow from the root page (default 0)
  -host string
        single host to test
  -hosts string
        filename with hosts, one host per line.
  -output string
        output format (stdout|csv|json) (default "stdout")
  -search
        searches all urls with same base domain (i.e. example.com and sub.example.com) (default true)
  -silent
	    avoid printing header (default false)
  -update
        update apps file
  -worker int
        number of worker (default 4)
```

para hacer un escaneo de tecnologias debomos usar el parametro `-host` para indicarle el objetivo un ejemplo de uso seria el siguiente

```
webanalyze -host facebook.com -crawl 1
```

con el `-crawl 1` le indicamos que siga un enlace apartir del directorio root

* [whatweb ](https://github.com/urbanadventurer/whatweb)-> WhatWeb identifica sitios web. Su objetivo es responder a la pregunta "¿Qué es ese sitio web?". WhatWeb reconoce las tecnologías web, incluidos los sistemas de administración de contenido (CMS), las plataformas de blogs, los paquetes de estadísticas/análisis, las bibliotecas de JavaScript, los servidores web y los dispositivos integrados. WhatWeb tiene más de 1800 complementos, cada uno para reconocer algo diferente. WhatWeb también identifica números de versión, direcciones de correo electrónico, ID de cuenta, módulos de marco web, errores de SQL y más.

#### USO&#x20;

```
Usage: whatweb [options] <URLs>

TARGET SELECTION:
  <TARGETs>             Enter URLs, hostnames, IP addresses, filenames or
                        IP ranges in CIDR, x.x.x-x, or x.x.x.x-x.x.x.x
                        format.
  --input-file=FILE, -i Read targets from a file. You can pipe
                        hostnames or URLs directly with -i /dev/stdin.

TARGET MODIFICATION:
  --url-prefix          Add a prefix to target URLs.
  --url-suffix          Add a suffix to target URLs.
  --url-pattern         Insert the targets into a URL. Requires --input-file,
                        eg. www.example.com/%insert%/robots.txt 

AGGRESSION:
  The aggression level controls the trade-off between speed/stealth and
  reliability.
  --aggression, -a=LEVEL Set the aggression level. Default: 1.
  Aggression levels are:
  1. Stealthy   Makes one HTTP request per target. Also follows redirects.
  3. Aggressive If a level 1 plugin is matched, additional requests will be
      made.
  4. Heavy      Makes a lot of HTTP requests per target. Aggressive tests from
      all plugins are used for all URLs.

HTTP OPTIONS:
  --user-agent, -U=AGENT Identify as AGENT instead of WhatWeb/0.5.5.
  --header, -H          Add an HTTP header. eg "Foo:Bar". Specifying a default
                        header will replace it. Specifying an empty value, eg.
                        "User-Agent:" will remove the header.
  --follow-redirect=WHEN Control when to follow redirects. WHEN may be `never',
                        `http-only', `meta-only', `same-site', or `always'.
                        Default: always.
  --max-redirects=NUM   Maximum number of contiguous redirects. Default: 10.

AUTHENTICATION:
  --user, -u=<user:password> HTTP basic authentication.
  --cookie, -c=COOKIES  Provide cookies, e.g. 'name=value; name2=value2'.
  --cookiejar=FILE      Read cookies from a file.

PROXY:
  --proxy           <hostname[:port]> Set proxy hostname and port.
                    Default: 8080.
  --proxy-user      <username:password> Set proxy user and password.

PLUGINS:
  --list-plugins, -l            List all plugins.
  --info-plugins, -I=[SEARCH]   List all plugins with detailed information.
                                Optionally search with keywords in a comma
                                delimited list.
  --search-plugins=STRING       Search plugins for a keyword.
  --plugins, -p=LIST  Select plugins. LIST is a comma delimited set of 
                      selected plugins. Default is all.
                      Each element can be a directory, file or plugin name and
                      can optionally have a modifier, eg. + or -
                      Examples: +/tmp/moo.rb,+/tmp/foo.rb
                      title,md5,+./plugins-disabled/
                      ./plugins-disabled,-md5
                      -p + is a shortcut for -p +plugins-disabled.

  --grep, -g=STRING|REGEXP      Search for STRING or a Regular Expression. Shows 
                                only the results that match.
                                Examples: --grep "hello"
                                --grep "/he[l]*o/"
  --custom-plugin=DEFINITION\tDefine a custom plugin named Custom-Plugin,
  --custom-plugin=DEFINITION  Define a custom plugin named Custom-Plugin,
                        Examples: ":text=>'powered by abc'"
                        ":version=>/powered[ ]?by ab[0-9]/"
                        ":ghdb=>'intitle:abc \"powered by abc\"'"
                        ":md5=>'8666257030b94d3bdb46e05945f60b42'"
  --dorks=PLUGIN        List Google dorks for the selected plugin.

OUTPUT:
  --verbose, -v         Verbose output includes plugin descriptions. Use twice
                        for debugging.
  --colour,--color=WHEN control whether colour is used. WHEN may be `never',
                        `always', or `auto'.
  --quiet, -q           Do not display brief logging to STDOUT.
  --no-errors           Suppress error messages.

LOGGING:
  --log-brief=FILE        Log brief, one-line output.
  --log-verbose=FILE      Log verbose output.
  --log-errors=FILE       Log errors.
  --log-xml=FILE          Log XML format.
  --log-json=FILE         Log JSON format.
  --log-sql=FILE          Log SQL INSERT statements.
  --log-sql-create=FILE   Create SQL database tables.
  --log-json-verbose=FILE Log JSON Verbose format.
  --log-magictree=FILE    Log MagicTree XML format.
  --log-object=FILE       Log Ruby object inspection format.
  --log-mongo-database    Name of the MongoDB database.
  --log-mongo-collection  Name of the MongoDB collection. Default: whatweb.
  --log-mongo-host        MongoDB hostname or IP address. Default: 0.0.0.0.
  --log-mongo-username    MongoDB username. Default: nil.
  --log-mongo-password    MongoDB password. Default: nil.  
  --log-elastic-index     Name of the index to store results. Default: whatweb 
  --log-elastic-host      Host:port of the elastic http interface. Default: 127.0.0.1:9200
  
PERFORMANCE & STABILITY:
  --max-threads, -t       Number of simultaneous threads. Default: 25.
  --open-timeout          Time in seconds. Default: 15.
  --read-timeout          Time in seconds. Default: 30.
  --wait=SECONDS          Wait SECONDS between connections.
                          This is useful when using a single thread.

HELP & MISCELLANEOUS:
  --short-help            Short usage help.
  --help, -h              Complete usage help.
  --debug                 Raise errors in plugins.
  --version               Display version information. (WhatWeb 0.5.5).

EXAMPLE USAGE:
* Scan example.com.
  ./whatweb example.com
* Scan reddit.com slashdot.org with verbose plugin descriptions.
  ./whatweb -v reddit.com slashdot.org
* An aggressive scan of wired.com detects the exact version of WordPress.
  ./whatweb -a 3 www.wired.com
* Scan the local network quickly and suppress errors.
  whatweb --no-errors 192.168.0.0/24
* Scan the local network for https websites.
  whatweb --no-errors --url-prefix https:// 192.168.0.0/24
* Scan for crossdomain policies in the Alexa Top 1000.
  ./whatweb -i plugin-development/alexa-top-100.txt \
  --url-suffix /crossdomain.xml -p crossdomain_xml

```

como podeis ver whatweb contiene muchos parametros para poder personalizar nuestro comando o mejor dicho ajustarlo a cada situacion pero una forma simple de escanear las tecnologias que corren en dicho dominio se puede usar

```
whatweb -v facebook.com
```

ya veis que es bastante simple la forma de escanear las tecnologias con el     `-v` le indicamos el modo verbose y seguido el dominio

WhatWeb presenta varios niveles de agresión. De forma predeterminada, el nivel de agresivo se establece en 1 (sigiloso), lo que envía una única solicitud HTTP GET y también sigue las redirecciones pero hay hasta 4 modos de agresivo que se indica con el parametro `-a`

```
whatweb -a 4 facebook.com
```

### Descubrimiento de contenido

* [gobuster ](https://github.com/OJ/gobuster)->  gobuster es una herramienta utilizada para fuerza bruta de URL  , subdominios, nombres de host, servidores TFTP para utilizar cada uno de ellos ensisten modos que se añaden despues de gobuester en el comando&#x20;

```
dir: el directorio clásico en modo de fuerza bruta
dns: modo de fuerza bruta del subdominio DNS
s3: enumere los cubos S3 abiertos y busque la existencia y las listas de cubos
gcs - Enumerar cubos abiertos de Google Cloud
vhost: modo de fuerza bruta de host virtual (¡no es lo mismo que DNS!)
fuzz - algo de fuzzing básico, reemplaza la FUZZpalabra clave
tftp - archivos tftp de fuerza bruta
```

#### USO

```
Uses directory/file enumeration mode

Usage:
  gobuster dir [flags]

Flags:
  -f, --add-slash                       Append / to each request
  -c, --cookies string                  Cookies to use for the requests
  -d, --discover-backup                 Also search for backup files by appending multiple backup extensions
      --exclude-length ints             exclude the following content length (completely ignores the status). Supply multiple times to exclude multiple sizes.
  -e, --expanded                        Expanded mode, print full URLs
  -x, --extensions string               File extension(s) to search for
  -r, --follow-redirect                 Follow redirects
  -H, --headers stringArray             Specify HTTP headers, -H 'Header1: val1' -H 'Header2: val2'
  -h, --help                            help for dir
      --hide-length                     Hide the length of the body in the output
  -m, --method string                   Use the following HTTP method (default "GET")
  -n, --no-status                       Don't print status codes
  -k, --no-tls-validation               Skip TLS certificate verification
  -P, --password string                 Password for Basic Auth
      --proxy string                    Proxy to use for requests [http(s)://host:port]
      --random-agent                    Use a random User-Agent string
      --retry                           Should retry on request timeout
      --retry-attempts int              Times to retry on request timeout (default 3)
  -s, --status-codes string             Positive status codes (will be overwritten with status-codes-blacklist if set)
  -b, --status-codes-blacklist string   Negative status codes (will override status-codes if set) (default "404")
      --timeout duration                HTTP Timeout (default 10s)
  -u, --url string                      The target URL
  -a, --useragent string                Set the User-Agent string (default "gobuster/3.2.0")
  -U, --username string                 Username for Basic Auth

Global Flags:
      --delay duration    Time each thread waits between requests (e.g. 1500ms)
      --no-color          Disable color output
      --no-error          Don't display errors
  -z, --no-progress       Don't display progress
  -o, --output string     Output file to write results to (defaults to stdout)
  -p, --pattern string    File containing replacement patterns
  -q, --quiet             Don't print the banner and other noise
  -t, --threads int       Number of concurrent threads (default 10)
  -v, --verbose           Verbose output (errors)
  -w, --wordlist string   Path to the wordlist
```

un ejemplo de uso de gobuster para descubrir directorios es&#x20;

```
gobuster dir -u http://google.com -t 50 -w wordlist -x php,html
```

este comando lo que haria seira ejecutar gobuster especificamos que queremos buscar directorios le pasamos la URL le decimos con -t el numero de hilos que queremos con `-w` la wordlist y con `-w` que busque solo los archivos php y html

tambien lo podemos usar para el descubrimento DNS

```
Uses DNS subdomain enumeration mode

Usage:
  gobuster dns [flags]

Flags:
  -d, --domain string      The target domain
  -h, --help               help for dns
  -r, --resolver string    Use custom DNS server (format server.com or server.com:port)
  -c, --show-cname         Show CNAME records (cannot be used with '-i' option)
  -i, --show-ips           Show IP addresses
      --timeout duration   DNS resolver timeout (default 1s)
      --wildcard           Force continued operation when wildcard found

Global Flags:
      --delay duration    Time each thread waits between requests (e.g. 1500ms)
      --no-color          Disable color output
      --no-error          Don't display errors
  -z, --no-progress       Don't display progress
  -o, --output string     Output file to write results to (defaults to stdout)
  -p, --pattern string    File containing replacement patterns
  -q, --quiet             Don't print the banner and other noise
  -t, --threads int       Number of concurrent threads (default 10)
  -v, --verbose           Verbose output (errors)
  -w, --wordlist string   Path to the wordlist
```

el ejemplo facil de uso es&#x20;

```
gobuster dns -d dominio.com -t 50 -w wordlist
```

tambien se puede usar par el descubrimiento de vhost&#x20;

```
Uses VHOST enumeration mode (you most probably want to use the IP address as the URL parameter)

Usage:
  gobuster vhost [flags]

Flags:
      --append-domain         Append main domain from URL to words from wordlist. Otherwise the fully qualified domains need to be specified in the wordlist.
  -c, --cookies string        Cookies to use for the requests
      --domain string         the domain to append when using an IP address as URL. If left empty and you specify a domain based URL the hostname from the URL is extracted
      --exclude-length ints   exclude the following content length (completely ignores the status). Supply multiple times to exclude multiple sizes.
  -r, --follow-redirect       Follow redirects
  -H, --headers stringArray   Specify HTTP headers, -H 'Header1: val1' -H 'Header2: val2'
  -h, --help                  help for vhost
  -m, --method string         Use the following HTTP method (default "GET")
  -k, --no-tls-validation     Skip TLS certificate verification
  -P, --password string       Password for Basic Auth
      --proxy string          Proxy to use for requests [http(s)://host:port]
      --random-agent          Use a random User-Agent string
      --retry                 Should retry on request timeout
      --retry-attempts int    Times to retry on request timeout (default 3)
      --timeout duration      HTTP Timeout (default 10s)
  -u, --url string            The target URL
  -a, --useragent string      Set the User-Agent string (default "gobuster/3.2.0")
  -U, --username string       Username for Basic Auth

Global Flags:
      --delay duration    Time each thread waits between requests (e.g. 1500ms)
      --no-color          Disable color output
      --no-error          Don't display errors
  -z, --no-progress       Don't display progress
  -o, --output string     Output file to write results to (defaults to stdout)
  -p, --pattern string    File containing replacement patterns
  -q, --quiet             Don't print the banner and other noise
  -t, --threads int       Number of concurrent threads (default 10)
  -v, --verbose           Verbose output (errors)
  -w, --wordlist string   Path to the wordlist
```

```
gobuster vhost -h https;//facebook.com -w comon-vhost.txt
```

* [feroxbuster ](https://github.com/epi052/feroxbuster)-> feroxbuster es una muy buena herramienta para el descubrimiento de directorios que en lo personal es de mis favoritas por su facil uso y su forma de representar los resultados

#### USO

```
feroxbuster -u http://127.1 -x pdf -x js,html -x php txt json,docx
```

El comando anterior agrega .pdf, .js, .html, .php, .txt, .json y .docx a cada URL

Todos los métodos anteriores (múltiples banderas, separados por espacios, separados por comas, etc.) son válidos e intercambiables. Lo mismo ocurre con las direcciones URL, los encabezados, los códigos de estado, las consultas y los filtros de tamaño.

**Incluir encabezados**

```
feroxbuster -u http://127.1 -H Accept:application/json "Authorization: Bearer {token}"
```

#### IPv6, escaneo no recursivo con registro de nivel INFO habilitado

```
/feroxbuster -u http://[::1] --no-recursion -vv
```

#### Tráfico de proxy a través de Burp

```
feroxbuster -u http://127.1 --insecure --proxy http://127.0.0.1:8080
```

#### Pase el token de autenticación a través del parámetro de consulta

```
feroxbuster -u http://127.1 --query token=0123456789ABCDEF
```

* [dirsearch ](https://github.com/maurosoria/dirsearch)-> Escáner de rutas web

#### USO

```
dirsearch -e php -u http://facebook.com --exclude-status 403,404,401
```

tambien se pude usar fuerza bruta recursiva que consiste en que continua despues de los directorios encontrados  por ejemplo si encuentra un `/admin` este continua buscando `/admin/*`

```
dirsearch -e php,html,js -u http://facebook.com -r
```

Puede establecer la profundidad máxima de recurrencia con **--max-recursion-depth** y los códigos de estado para recurrir con **--recursion-status**

```
dirsearch -e html,php,js -u http://facebook.com -r --max-recursion-depth 3 --recursion-status 200-399
```

se pueden aplicar filtros a la busqueda utilizando  **`-i | --incluir-estado`**` ``y`` `**`-x | --exclude-status`** para seleccionar códigos de estado de respuesta permitidos y no permitidos

Para filtros más avanzados: **`--exclude-sizes`**` ``,`` `**`--exclude-texts`**` ``,`` `**`--exclude-regexps`**` ``,`` `**`--exclude-redirects`**` ``y`` `**`--exclude-response`**

### Parametros peticiones GET y POST

* [ParamPamPam ](https://github.com/Bo0oM/ParamPamPam)-> esta herramienta para el descubrimiento bruto de parámetros GET y POST.

#### Instalacion

```
git clone https://github.com/Bo0oM/ParamPamPam.git
cd ParamPamPam
pip3 install --no-cache-dir -r requirements.txt
```

#### USO

```
python3 parampp.py -u "https://facebook.com/login"
```

* [parameth ](https://github.com/maK-/parameth)-> esta herramienta se puede usar para descubrir parametros GET y POST de forma bruta A menudo, cuando busca archivos comunes en un directorio, puede identificar secuencias de comandos (por ejemplo, test.php) que parecen necesitar pasar un parámetro desconocido. Con suerte, esto puede ayudar a encontrarlos.

#### instalacion&#x20;

```
git clone https://github.com/maK-/parameth
pip install -u -r requirements.txt
```

#### USO

```
usage: parameth.py [-h] [-v] [-u URL] [-p PARAMS] [-H HEADER] [-a AGENT]
                   [-t THREADS] [-off VARIANCE] [-diff DIFFERENCE] [-o OUT]
                   [-P PROXY] [-x IGNORE] [-s SIZEIGNORE] [-d DATA]
                   [-i IGMETH] [-c COOKIE] [-T TIMEOUT]

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         Version Information
  -u URL, --url URL     Target URL
  -p PARAMS, --params PARAMS
                        Provide a list of parameters to scan for
  -H HEADER, --header HEADER
                        Add headers in format a:b c:d
  -a AGENT, --agent AGENT
                        Specify a user agent
  -t THREADS, --threads THREADS
                        Specify the number of threads.
  -off VARIANCE, --variance VARIANCE
                        The offset in difference to ignore (if dynamic pages)
  -diff DIFFERENCE, --difference DIFFERENCE
                        Percentage difference in response (recommended 95)
  -o OUT, --out OUT     Specify output file
  -P PROXY, --proxy PROXY
                        Specify a proxy in the form http|s://[IP]:[PORT]
  -x IGNORE, --ignore IGNORE
                        Specify a status to ignore eg. 404,302...
  -s SIZEIGNORE, --sizeignore SIZEIGNORE
                        Ignore responses of specified size
  -d DATA, --data DATA  Provide default post data (also taken from provided
                        url after ?)
  -i IGMETH, --igmeth IGMETH
                        Ignore GET or POST method. Specify g or p
  -c COOKIE, --cookie COOKIE
                        Specify Cookies
  -T TIMEOUT, --timeout TIMEOUT
                        Specify a timeout in seconds to wait between each
                        request
```

Las siguientes expresiones regulares pueden ser útiles para analizar `$_GET`o `$_POST`parámetros de la fuente:

> $> grep -rioP '$\_POST\[\s\*\["']\s\*\w+\s\*\["']\s\*]' PHPSOURCE | grep -oP '$\_POST\[\s\*\["']\s\*\w+\s\*\["']\s\*]' | sed -e "s/$\_POST\[\s\*\["']//g" -e "s/\s\*\['"]\s\*]//g" | sort -u > /tmp/outfile.txt

> $> grep -rioP '$\_GET\[\s\*\["']\s\*\w+\s\*\["']\s\*]' PHPSOURCE | grep -oP '$\_GET\[\s\*\["']\s\*\w+\s\*\["']\s\*]' | sed -e "s/$\_GET\[\s\*\["']//g" -e "s/\s\*\['"]\s\*]//g" | sort -u > /tmp/outfile.txt

### fuzzing

[wfuzz ](https://github.com/xmendez/wfuzz)-> Wfuzz ha sido creado para facilitar la tarea en la evaluación de aplicaciones web y se basa en un concepto simple: reemplaza cualquier referencia a la palabra clave FUZZ por el valor de una carga útil determinada.

#### USO

un uso basico de uso de wfuzz para descubrimiento de directorios es

```
wfuzz -w wordlist/general/common.txt --hc 404 http://facebook.com/FUZZ
```

En este comando le estamos indicando  con `-w` un wordlist  con `--hc` le indicamos que no nos muetre los 404 seguido ponemos la url segida de `/FUZZ`

wfuzz se puede emplear para hacer fuzzing de lo que necesitemo por ejemplo tambien se podria utilizar para descubrimiento de solicitudes POST

```
wfuzz -z file,wordlist/others/common_pass.txt -d "uname=FUZZ&pass=FUZZ"  --hc 302 http://facebook.com/userinfo.php
```

ademas tambien se puede emplear para ahacer fuzzing de headers, cookies,autentificacion

* [ffuf ](https://github.com/ffuf/ffuf)-> ffuf es una herramienta para hacer fuzzing escrita en go

#### USO&#x20;

```
Fuzz Faster U Fool - v2.0.0

HTTP OPTIONS:
  -H                  Header `"Name: Value"`, separated by colon. Multiple -H flags are accepted.
  -X                  HTTP method to use
  -b                  Cookie data `"NAME1=VALUE1; NAME2=VALUE2"` for copy as curl functionality.
  -d                  POST data
  -http2              Use HTTP2 protocol (default: false)
  -ignore-body        Do not fetch the response content. (default: false)
  -r                  Follow redirects (default: false)
  -recursion          Scan recursively. Only FUZZ keyword is supported, and URL (-u) has to end in it. (default: false)
  -recursion-depth    Maximum recursion depth. (default: 0)
  -recursion-strategy Recursion strategy: "default" for a redirect based, and "greedy" to recurse on all matches (default: default)
  -replay-proxy       Replay matched requests using this proxy.
  -sni                Target TLS SNI, does not support FUZZ keyword
  -timeout            HTTP request timeout in seconds. (default: 10)
  -u                  Target URL
  -x                  Proxy URL (SOCKS5 or HTTP). For example: http://127.0.0.1:8080 or socks5://127.0.0.1:8080

GENERAL OPTIONS:
  -V                  Show version information. (default: false)
  -ac                 Automatically calibrate filtering options (default: false)
  -acc                Custom auto-calibration string. Can be used multiple times. Implies -ac
  -ach                Per host autocalibration (default: false)
  -ack                Autocalibration keyword (default: FUZZ)
  -acs                Autocalibration strategy: "basic" or "advanced" (default: basic)
  -c                  Colorize output. (default: false)
  -config             Load configuration from a file
  -json               JSON output, printing newline-delimited JSON records (default: false)
  -maxtime            Maximum running time in seconds for entire process. (default: 0)
  -maxtime-job        Maximum running time in seconds per job. (default: 0)
  -noninteractive     Disable the interactive console functionality (default: false)
  -p                  Seconds of `delay` between requests, or a range of random delay. For example "0.1" or "0.1-2.0"
  -rate               Rate of requests per second (default: 0)
  -s                  Do not print additional information (silent mode) (default: false)
  -sa                 Stop on all error cases. Implies -sf and -se. (default: false)
  -scraperfile        Custom scraper file path
  -scrapers           Active scraper groups (default: all)
  -se                 Stop on spurious errors (default: false)
  -search             Search for a FFUFHASH payload from ffuf history
  -sf                 Stop when > 95% of responses return 403 Forbidden (default: false)
  -t                  Number of concurrent threads. (default: 40)
  -v                  Verbose output, printing full URL and redirect location (if any) with the results. (default: false)

MATCHER OPTIONS:
  -mc                 Match HTTP status codes, or "all" for everything. (default: 200,204,301,302,307,401,403,405,500)
  -ml                 Match amount of lines in response
  -mmode              Matcher set operator. Either of: and, or (default: or)
  -mr                 Match regexp
  -ms                 Match HTTP response size
  -mt                 Match how many milliseconds to the first response byte, either greater or less than. EG: >100 or <100
  -mw                 Match amount of words in response

FILTER OPTIONS:
  -fc                 Filter HTTP status codes from response. Comma separated list of codes and ranges
  -fl                 Filter by amount of lines in response. Comma separated list of line counts and ranges
  -fmode              Filter set operator. Either of: and, or (default: or)
  -fr                 Filter regexp
  -fs                 Filter HTTP response size. Comma separated list of sizes and ranges
  -ft                 Filter by number of milliseconds to the first response byte, either greater or less than. EG: >100 or <100
  -fw                 Filter by amount of words in response. Comma separated list of word counts and ranges

INPUT OPTIONS:
  -D                  DirSearch wordlist compatibility mode. Used in conjunction with -e flag. (default: false)
  -e                  Comma separated list of extensions. Extends FUZZ keyword.
  -ic                 Ignore wordlist comments (default: false)
  -input-cmd          Command producing the input. --input-num is required when using this input method. Overrides -w.
  -input-num          Number of inputs to test. Used in conjunction with --input-cmd. (default: 100)
  -input-shell        Shell to be used for running command
  -mode               Multi-wordlist operation mode. Available modes: clusterbomb, pitchfork, sniper (default: clusterbomb)
  -request            File containing the raw http request
  -request-proto      Protocol to use along with raw request (default: https)
  -w                  Wordlist file path and (optional) keyword separated by colon. eg. '/path/to/wordlist:KEYWORD'

OUTPUT OPTIONS:
  -debug-log          Write all of the internal logging to the specified file.
  -o                  Write output to file
  -od                 Directory path to store matched results to.
  -of                 Output file format. Available formats: json, ejson, html, md, csv, ecsv (or, 'all' for all formats) (default: json)
  -or                 Don't create the output file if we don't have results (default: false)

EXAMPLE USAGE:
  Fuzz file paths from wordlist.txt, match all responses but filter out those with content-size 42.
  Colored, verbose output.
    ffuf -w wordlist.txt -u https://example.org/FUZZ -mc all -fs 42 -c -v

  Fuzz Host-header, match HTTP 200 responses.
    ffuf -w hosts.txt -u https://example.org/ -H "Host: FUZZ" -mc 200

  Fuzz POST JSON data. Match all responses not containing text "error".
    ffuf -w entries.txt -u https://example.org/ -X POST -H "Content-Type: application/json" \
      -d '{"name": "FUZZ", "anotherkey": "anothervalue"}' -fr "error"

  Fuzz multiple locations. Match only responses reflecting the value of "VAL" keyword. Colored.
    ffuf -w params.txt:PARAM -w values.txt:VAL -u https://example.org/?PARAM=VAL -mr "VAL" -c

  More information and examples: https://github.com/ffuf/ffuf
```

reconocimiento de directorios sencillo pasandole una url y una wordlist

```
ffuf -c -w /path/to/wordlist -u https://target/FUZZ
```

con el `-c` le decimos que el output lo represente con colores lo que a lo personal me parece util  luego le pasamos la wordlist  y la url a la que al final se le añade el `/FUZZ` para indicar que es ahi donde queremos aplicar la fuerza bruta

Descubrimiento de vhost (sin registro DNS)

Suponiendo que el tamaño de respuesta predeterminado del host virtual es de 4242 bytes, podemos filtrar todas las respuestas de ese tamaño ( `-fs 4242`) mientras analizamos el encabezado Host:

```
ffuf -w /path/to/vhost/wordlist -u https://target -H "Host: FUZZ" -fs 4242
```

fuzzing de parametros GET

La fuzzing del nombre del parámetro GET es muy similar a la detección de directorios y funciona definiendo la `FUZZ`palabra clave como parte de la URL. Esto también supone un tamaño de respuesta de 4242 bytes para el nombre del parámetro GET no válido.

```
ffuf -w /path/to/paramnames.txt -u https://target/script.php?FUZZ=test_value -fs 4242
```

Fuzzing de datos POST

```
ffuf -w /path/to/postdata.txt -X POST -d "username=admin\&password=FUZZ" -u https://target/login.php -fc 401
```

Si no desea que ffuf se ejecute indefinidamente, puede usar el `-maxtime`. Esto detiene **todo el** proceso después de un tiempo determinado (en segundos).

```
ffuf -w /path/to/wordlist -u https://target/FUZZ -maxtime 60
```
